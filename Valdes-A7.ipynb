{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7.1 Autoencoder for Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have talked in lecture about how an Autoencoder nonlinearly reduces the dimensionality of data.  In this assignment you will \n",
    "1. load an autoencoder network already trained in the MNIST data,\n",
    "2. apply it to the MNIST training set to obtain the outputs of the units in the bottleneck layer as a new representation of each training set image with a greatly reduced dimensionality,\n",
    "3. Train a fully-connected classification network on this new representation.\n",
    "4. Report on the percent of training and testing images correctly classified.  Compare with the accuracy you get with the original images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [nn_torch.zip](https://www.cs.colostate.edu/~anderson/cs445/notebooks/nn_torch.zip) and extract the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import pickle\n",
    "import gzip\n",
    "import torch\n",
    "import neuralnetworks_torch as nntorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the MNIST data. You may download it here: [mnist.pkl.gz](http://deeplearning.net/data/mnist/mnist.pkl.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "Xtrain = train_set[0]\n",
    "Ttrain = train_set[1]\n",
    "\n",
    "Xtest = test_set[0]\n",
    "Ttest = test_set[1]\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the network saved in Lecture Notes 21, run the following code.  This loads the saved torch neural network that was trained in a GPU.  It loads the state of that net (its weights) into a new net of the same structure but allocated on the CPU.\n",
    "\n",
    "First download [mnist_autoencoder.pt](https://www.cs.colostate.edu/~anderson/cs445/notebooks/mnist_autoencoder.pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_in = Xtrain.shape[1]\n",
    "n_hiddens_per_layer = [500, 100, 50, 50, 20, 50, 50, 100, 500]\n",
    "nnet_autoencoder = nntorch.NeuralNetwork(n_in, n_hiddens_per_layer, n_in, device='cpu')\n",
    "nnet_autoencoder.standardize = ''\n",
    "\n",
    "nnet_autoencoder.load_state_dict(torch.load('mnist_autoencoder.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the output of the units in the middle hidden layer, run `use_to_middle` function implemented for you in `neuralnetworks_torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_reduced = nnet_autoencoder.use_to_middle(Xtrain)\n",
    "Xtrain_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while we are here, let's get the reduced representation of `Xtest` also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_reduced = nnet_autoencoder.use_to_middle(Xtest)\n",
    "Xtest_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "Your jobs are now to\n",
    "1. train one fully-connected classifier using `Xtrain_reduced` and `Ttrain` and test it with `Xtest_reduced` and `Ttest`, and\n",
    "2. train a second fully-connected classifier using `Xtrain` and `Ttrain` and test it with `Xtest` and `Ttest`.\n",
    "\n",
    "Try to find parameters (hidden network structure, number of epochs, and learning rate) for which the classifier given the reduced representation does almost as well as the other classifier with the original data. Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example for part of Step 1.  It shows a brief training session (small number of epochs and simple hidden layer structure) for using the reduced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: RMSE 2.138\n",
      "Epoch 10: RMSE 1.876\n",
      "Epoch 15: RMSE 1.555\n",
      "Epoch 20: RMSE 1.223\n",
      "Epoch 25: RMSE 0.945\n",
      "Epoch 30: RMSE 0.746\n",
      "Epoch 35: RMSE 0.612\n",
      "Epoch 40: RMSE 0.523\n",
      "Epoch 45: RMSE 0.463\n",
      "Epoch 50: RMSE 0.422\n",
      "% Correct  Ttest 88.82\n"
     ]
    }
   ],
   "source": [
    "n_in = Xtrain_reduced.shape[1]\n",
    "reduced_classifier = nntorch.NeuralNetwork_Classifier(n_in, [50], 10, device='cpu')\n",
    "\n",
    "n_epochs = 50\n",
    "reduced_classifier.train(Xtrain_reduced, Ttrain, n_epochs, 0.01, method='adam', standardize='')\n",
    "\n",
    "Classes, _ = reduced_classifier.use(Xtest_reduced)\n",
    "\n",
    "def percent_correct(Predicted, Target):\n",
    "    return 100 * np.mean(Predicted == Target)\n",
    "\n",
    "print(f'% Correct  Ttest {percent_correct(Classes, Ttest):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: RMSE 1.989\n",
      "Epoch 10: RMSE 1.202\n",
      "Epoch 15: RMSE 0.768\n",
      "Epoch 20: RMSE 0.596\n",
      "Epoch 25: RMSE 0.496\n",
      "Epoch 30: RMSE 0.432\n",
      "Epoch 35: RMSE 0.393\n",
      "Epoch 40: RMSE 0.362\n",
      "Epoch 45: RMSE 0.338\n",
      "Epoch 50: RMSE 0.321\n",
      "% Correct  Ttest 90.88\n"
     ]
    }
   ],
   "source": [
    "#reduced data experiment 1: bigger network using reduced data\n",
    "n_in = Xtrain_reduced.shape[1]\n",
    "#NeuralNetwork(n_in, n_hiddens_per_layer, n_in, device='cpu')\n",
    "reduced_classifier = nntorch.NeuralNetwork_Classifier(n_in, [50, 50, 50], 10, device='cpu')\n",
    "\n",
    "n_epochs = 50\n",
    "reduced_classifier.train(Xtrain_reduced, Ttrain, n_epochs, 0.01, method='adam', standardize='')\n",
    "\n",
    "Classes, _ = reduced_classifier.use(Xtest_reduced)\n",
    "\n",
    "def percent_correct(Predicted, Target):\n",
    "    return 100 * np.mean(Predicted == Target)\n",
    "\n",
    "print(f'% Correct  Ttest {percent_correct(Classes, Ttest):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: RMSE 0.643\n",
      "Epoch 20: RMSE 0.378\n",
      "Epoch 30: RMSE 0.316\n",
      "Epoch 40: RMSE 0.274\n",
      "Epoch 50: RMSE 0.244\n",
      "Epoch 60: RMSE 0.221\n",
      "Epoch 70: RMSE 0.200\n",
      "Epoch 80: RMSE 0.178\n",
      "Epoch 90: RMSE 0.156\n",
      "Epoch 100: RMSE 0.136\n",
      "% Correct  Ttest 95.71\n"
     ]
    }
   ],
   "source": [
    "#reduced data experiment 2: even bigger network trained for more epochs\n",
    "n_in = Xtrain_reduced.shape[1]\n",
    "#NeuralNetwork(n_in, n_hiddens_per_layer, n_in, device='cpu')\n",
    "reduced_classifier = nntorch.NeuralNetwork_Classifier(n_in, [100, 100, 100], 10, device='cpu')\n",
    "\n",
    "n_epochs = 100\n",
    "reduced_classifier.train(Xtrain_reduced, Ttrain, n_epochs, 0.01, method='adam', standardize='')\n",
    "\n",
    "Classes, _ = reduced_classifier.use(Xtest_reduced)\n",
    "\n",
    "def percent_correct(Predicted, Target):\n",
    "    return 100 * np.mean(Predicted == Target)\n",
    "\n",
    "print(f'% Correct  Ttest {percent_correct(Classes, Ttest):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: RMSE 0.637\n",
      "Epoch 20: RMSE 0.327\n",
      "Epoch 30: RMSE 0.246\n",
      "Epoch 40: RMSE 0.196\n",
      "Epoch 50: RMSE 0.162\n",
      "Epoch 60: RMSE 0.136\n",
      "Epoch 70: RMSE 0.115\n",
      "Epoch 80: RMSE 0.098\n",
      "Epoch 90: RMSE 0.084\n",
      "Epoch 100: RMSE 0.072\n",
      "% Correct  Ttest 96.11\n"
     ]
    }
   ],
   "source": [
    "#part 2: using original data, larger network than original with more epochs\n",
    "#changed: n_in to Xtrain.shape[1]\n",
    "n_in = Xtrain.shape[1]\n",
    "#NeuralNetwork(n_in, n_hiddens_per_layer, n_in, device='cpu')\n",
    "reduced_classifier = nntorch.NeuralNetwork_Classifier(n_in, [50, 50], 10, device='cpu')\n",
    "\n",
    "n_epochs = 100\n",
    "reduced_classifier.train(Xtrain, Ttrain, n_epochs, 0.01, method='adam', standardize='')\n",
    "\n",
    "Classes, _ = reduced_classifier.use(Xtest)\n",
    "\n",
    "def percent_correct(Predicted, Target):\n",
    "    return 100 * np.mean(Predicted == Target)\n",
    "\n",
    "print(f'% Correct  Ttest {percent_correct(Classes, Ttest):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results\n",
    "\n",
    "I found that in order for the network using the reduced data to do as well as a normal network it needed to have a considerably larger network.  In particular I found that when using the reduced data, a network with hidden layers [100, 100, 100], trained for 100 epochs and using a learning rate of 0.01 resulted in a percent correct on Ttest of 95.71.  Using the normal data, a network with hidden layers [50, 50] trained for 100 epochs with a learning rate of 0.01 resulted in a percent correct on Ttest of 96.11.  From this, it can be seen that a network using reduced data requires more resources such as network size or number of epochs in order to come close to the accuracy of a network using normal data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "For 1 point of extra credit repeat this assignment using a second data set, one that we have not used in class before. This will require you to to train a new autoencoder net to use for this part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
